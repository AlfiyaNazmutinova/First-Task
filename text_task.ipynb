{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на тексты\n",
    "\n",
    "1) Скачать fetch_20newsgroups из sklearn.\n",
    "\n",
    "2) Написать функцию предобработки, которая лемматизирует их (для англ это nltk/spacy, пример nltk - http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization)\n",
    "\n",
    "3) Сравнить качество CountVectorizer, TfIdfVectorizer, HashingVectorizer (1-2 значение размера хэша). Классификатор выберете любой, можно логистическую регрессию или бустинг.\n",
    "\n",
    "4) Написать функцию, которая делает из предложения один вектор - взвешенные word2vec представления слов по их tf-idf весам.\n",
    "\n",
    "5) В качестве признаков взять резуьтат функции 4 и посмотреть качество/сравнить со взвешенным по среднему а не tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat = ['talk.religion.misc','soc.religion.christian']\n",
    "#ds_train=datasets.load_files('/Users/a1/20news-bydate-train',categories = cat, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['talk.religion.misc','soc.religion.christian'] # Выбираем две категории для классификации\n",
    "# Данные для обучения\n",
    "ds_train = fetch_20newsgroups(subset='train', categories = cat, shuffle=True, random_state=1\n",
    "                                  ,remove=('headers', 'footers', 'quotes'))\n",
    "# Данне для проверки\n",
    "ds_test = fetch_20newsgroups(subset='test', categories = cat, shuffle=True, random_state=1\n",
    "                                  ,remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#приведение к нижнему регистру\n",
    "def get_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаление пунктуации\n",
    "def remove_punctuation(data):\n",
    "    exclude = r'[!?,.:;()\"*/\\\\_]'\n",
    "    s = re.sub(exclude, ' ',data)\n",
    "    s = re.sub('-', '', s)\n",
    "    return ' '.join(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#приведение англ. слов к нормальной форме\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "NORMAL_FORM = {}\n",
    "\n",
    "# сначала научимся работать со словами\n",
    "def get_word_normal_form(word):\n",
    "    if word not in NORMAL_FORM:\n",
    "        NORMAL_FORM[word] = st.stem(word)\n",
    "    return NORMAL_FORM[word]\n",
    "\n",
    "# а затем с текстом\n",
    "def get_sentence_normal_form(sentence):\n",
    "    return ' '.join(get_word_normal_form(word) for word in sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатываем обучающие данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "n = len(ds_train.data)\n",
    "for i in range(n):\n",
    "    ds_train.data[i] = get_lower(str(ds_train.data[i]))\n",
    "    ds_train.data[i] = remove_punctuation(str(ds_train.data[i]))\n",
    "    ds_train.data[i] = get_sentence_normal_form(str(ds_train.data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i don't see the effort to equ salv with parad rath i see imply the fact that on thos who ar sav may ent parad\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.data[1]\n",
    "#ds_train.target[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатываем тестовые файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(ds_test.data)\n",
    "for i in range(n):\n",
    "    ds_test.data[i] = remove_punctuation(str(ds_test.data[i]))\n",
    "    ds_test.data[i] = get_sentence_normal_form(str(ds_test.data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from dhammers@pacific david hammersl how do you mormon reconcil the ide of etern marry with christ's stat that in the resurrect peopl wil neith marry nor be giv in marry luk chapt 20 footnot in som bibl ref thi vers to the book of tobit tobit is in the septuagint goodspee publ it in a book cal the apocryph most any bookst wil hav thi at any rat the jew of christ's day had thi book it is a story most cent around the son of tobit who was nam tobia ther was a young lady sarah who had ent the brid chamb with sev broth in success the broth al died in the chamb bef consum the marry tobia was entitl to hav sarah for his wif 3 17 becaus tobia was her on rel and she was destin for [tobias] from the begin 6 17 tobia took her to wif and was abl to consum the marry the sev husband would not hav her as a partn in heav that doe not elimin tobia her eigh husband tobit is a fun and interest story to read it's kind of a myth rom it's a littl short than esth the lds also hav scriptures that parallel and ampl luk 20 most not doctrin and cov 132 1516 theref if a man marry him a wif in the world and he marry her not by me nor by my word and he cov with her so long as he is in the world and she with him their cov and marry ar not of forc when they ar dead and when they ar out of the world theref they ar not bound by any law when they ar out of the world theref when they ar out of the world they neith marry nor ar giv in marry but ar appoint angel in heav which angel ar min serv to min for thos who ar worthy of a far mor and an excess and an etern weight of glory\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 9937)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(ds_train.data)\n",
    "ds_train_counts = count_vect.transform(ds_train.data)\n",
    "ds_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 9937)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ds_test_counts = count_vect.transform(ds_test.data)\n",
    "ds_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.78      0.78       398\n",
      "          1       0.66      0.66      0.66       251\n",
      "\n",
      "avg / total       0.74      0.73      0.74       649\n",
      "\n",
      "[[311  87]\n",
      " [ 85 166]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(ds_train_counts, ds_train.target)\n",
    "print(model)\n",
    "#make predictions\n",
    "expected = ds_test.target\n",
    "predicted = model.predict(ds_test_counts)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 9937)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "ds_train_tf = vectorizer.fit_transform(ds_train.data)\n",
    "ds_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 9937)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ds_test_tf = vectorizer.transform(ds_test.data)\n",
    "ds_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72321875796059842"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(ds_train_tf, ds_train.target)\n",
    "pred = clf.predict(ds_test_tf)\n",
    "metrics.f1_score(ds_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.91      0.80       398\n",
      "          1       0.76      0.43      0.55       251\n",
      "\n",
      "avg / total       0.73      0.73      0.71       649\n",
      "\n",
      "[[363  35]\n",
      " [142 109]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(ds_train_tf, ds_train.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = ds_test.target\n",
    "predicted = model.predict(ds_test_tf)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 1048576)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer()\n",
    "ds_train_hv=hv.fit_transform(ds_train.data)\n",
    "ds_train_hv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 1048576)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "ds_test_hv=hv.transform(ds_test.data)\n",
    "ds_test_hv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.90      0.79       398\n",
      "          1       0.71      0.39      0.51       251\n",
      "\n",
      "avg / total       0.71      0.70      0.68       649\n",
      "\n",
      "[[358  40]\n",
      " [152  99]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(ds_train_hv, ds_train.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = ds_test.target\n",
    "predicted = model.predict(ds_test_hv)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v = list(sentence.split() for sentence in ds_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model = KeyedVectors.load_word2vec_format('/Users/a1/DA/base/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#w2v_model = KeyedVectors.load_word2vec_format(data_w2v, binary=True)\n",
    "w2v_model = word2vec.Word2Vec(data_w2v, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('should', 0.9904413819313049),\n",
       " ('may', 0.9839595556259155),\n",
       " ('if', 0.9765948057174683),\n",
       " ('would', 0.9764683842658997),\n",
       " ('do', 0.9711072444915771),\n",
       " ('think', 0.9689772725105286),\n",
       " ('see', 0.9684304594993591),\n",
       " (\"don't\", 0.9643958806991577),\n",
       " ('am', 0.9628705978393555),\n",
       " ('wil', 0.9605115056037903)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('can')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('jes', 0.9331137537956238),\n",
       " ('god', 0.896336555480957),\n",
       " ('his', 0.8932311534881592),\n",
       " ('christ', 0.879121720790863),\n",
       " ('himself', 0.8612052798271179),\n",
       " ('fath', 0.8582339286804199),\n",
       " ('son', 0.8419334888458252),\n",
       " ('sin', 0.8396385312080383),\n",
       " ('lord', 0.8391227126121521),\n",
       " (\"father's\", 0.8309696912765503)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=['he', 'man'], negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### средневзвешенная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence):\n",
    "    words = list(filter(lambda x: x in w2v_model, sentence.split()))\n",
    "    if len(words) == 0:\n",
    "        return None\n",
    "    vectors = list(map(lambda x: w2v_model[x], words))\n",
    "    sen_embedding = np.mean(vectors, axis=0) #max,median,min...\n",
    "    sen_embedding /= np.linalg.norm(sen_embedding, axis=0)\n",
    "    return sen_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i don't see the effort to equ salv with parad rath i see imply the fact that on thos who ar sav may ent parad\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"don't\",\n",
       " 'see',\n",
       " 'the',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'equ',\n",
       " 'salv',\n",
       " 'with',\n",
       " 'parad',\n",
       " 'rath',\n",
       " 'i',\n",
       " 'see',\n",
       " 'imply',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'on',\n",
       " 'thos',\n",
       " 'who',\n",
       " 'ar',\n",
       " 'sav',\n",
       " 'may',\n",
       " 'ent',\n",
       " 'parad']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w2v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  9.31517407e-02,   2.66948715e-02,  -7.80700594e-02,\n",
       "        -7.55531481e-03,   2.37862542e-01,   1.01591842e-02,\n",
       "         7.13977292e-02,  -1.67431254e-02,  -8.62257630e-02,\n",
       "        -1.09392866e-01,   8.49180296e-02,   1.94837525e-01,\n",
       "         1.35864586e-01,  -3.91463563e-02,  -1.86599605e-02,\n",
       "        -1.26634434e-01,  -2.02136770e-01,   3.63745540e-02,\n",
       "         1.37989834e-01,   1.58018619e-01,   5.66254295e-02,\n",
       "        -8.59327316e-02,   1.29342541e-01,  -3.04575581e-02,\n",
       "        -3.40347365e-02,   1.09401196e-02,   7.19777048e-02,\n",
       "         2.15591509e-02,   2.01317333e-02,  -7.02017769e-02,\n",
       "         6.56886846e-02,   5.69463633e-02,  -1.09685689e-01,\n",
       "        -1.07391357e-01,  -1.12998508e-01,   3.60714421e-02,\n",
       "        -5.68903759e-02,   4.16043364e-02,   1.09745629e-01,\n",
       "        -2.08835423e-04,   6.61634188e-03,   1.66102499e-02,\n",
       "         8.47644284e-02,  -1.39446659e-02,   1.48966789e-01,\n",
       "        -5.57388365e-02,   1.01096697e-01,  -1.45959884e-01,\n",
       "        -1.88850373e-01,   3.78392115e-02,   6.66332543e-02,\n",
       "        -4.55560312e-02,   6.65039197e-02,  -1.11033265e-02,\n",
       "        -6.09943680e-02,   1.18696012e-01,   1.18544260e-02,\n",
       "        -7.92025700e-02,  -1.23475209e-01,   1.55741930e-01,\n",
       "         1.77521244e-01,   3.15313265e-02,   6.59232214e-02,\n",
       "         4.77994084e-02,  -1.04649544e-01,  -1.68485060e-01,\n",
       "         3.09930500e-02,   9.63315144e-02,   9.68695269e-04,\n",
       "         1.00685693e-01,  -1.47113234e-01,   1.45038553e-02,\n",
       "        -6.29276186e-02,  -3.12010869e-02,   7.24819675e-02,\n",
       "         7.07825422e-02,   1.57398880e-01,  -6.77996024e-04,\n",
       "         5.63095473e-02,   3.51668447e-02,   1.45041063e-01,\n",
       "        -1.14173584e-01,  -1.60065919e-01,   2.32655145e-02,\n",
       "        -1.29777431e-01,   3.31617296e-02,  -3.34519297e-02,\n",
       "         2.83356100e-01,  -1.37526453e-01,  -7.72666484e-02,\n",
       "        -2.77551729e-03,   3.66548565e-03,   1.17453381e-01,\n",
       "        -3.05588227e-02,  -1.55687863e-02,   8.76946822e-02,\n",
       "        -1.13647267e-01,   1.57869488e-01,  -1.59315914e-01,\n",
       "        -1.43806800e-01], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_embedding(data_w2v[1])\n",
    "get_embedding(str(ds_train.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.save(\"file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### взвешенные word2vec представления слов по их tf-idf весам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_embedding(data, sentence):\n",
    "    vectors=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_data =  vectorizer.fit_transform(data)\n",
    "    vec=vectorizer.get_feature_names()\n",
    "    \n",
    "    words = list(filter(lambda x: x in w2v_model, sentence.split()))\n",
    "    if len(words) == 0:\n",
    "        return None\n",
    "\n",
    "    for word in words:\n",
    "            try:\n",
    "                vectors.append(tfidf_data[data.index(sentence), vec.index(word)] * w2v_model[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    sen_embedding = np.mean(vectors, axis=0) #max,median,min...\n",
    "    sen_embedding /= np.linalg.norm(sen_embedding, axis=0)\n",
    "    return sen_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'i' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-03700bc7a39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#почему то не попадают местоимения((\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-06c26203f848>\u001b[0m in \u001b[0;36mtfidf_embedding\u001b[0;34m(data, sentence)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'i' is not in list"
     ]
    }
   ],
   "source": [
    "#почему то не попадают местоимения((\n",
    "tfidf_embedding(ds_train.data,str(ds_train.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
